# Vanguard Digital Experiment â€“ A/B Testing Analysis

## ğŸ“˜ Overview
This project analyzes Vanguardâ€™s digital A/B experiment evaluating whether a redesigned user interface (UI) improves client engagement and completion of a multi-step online process.  
Using demographics, digital activity logs, and experiment assignments, this analysis measures the redesignâ€™s effectiveness and identifies opportunities for further improvement.

---

## ğŸ“‚ Data Sources
- **Client Profiles** â€“ Demographic details including age, balance, tenure, and gender.  
- **Digital Footprints** â€“ Step-level interaction data capturing user behavior.  
- **Experiment Roster** â€“ Control vs. Test group assignments for A/B evaluation.

After cleaning (including removing NaN values in the `variation` field) and merging all datasets, the final dataset contained **50,500 unique client IDs**.

---

## ğŸ‘¥ Client Behavior Insights
Demographic patterns across Test and Control groups were consistent:

- **Age:** Most active users were between **30â€“49** and **50â€“69**.  
- **Balance:** Highest engagement from clients with **$10kâ€“$50k**.  
- **Tenure:** Strongest activity from users with **6â€“10 years** of tenure.

These trends help contextualize overall experiment performance.

---

## ğŸ“Š Key Performance Indicators (KPIs)
To evaluate the designâ€™s success, three metrics were prioritized:

1. **Completion Rate** â€“ % of users completing the final "confirm" step  
2. **Time Spent per Step** â€“ Average time per stage of the process  
3. **Error Rate** â€“ % of sessions where errors occurred  

---

## ğŸ§ª Hypothesis Testing Summary

### **1. Completion Rate (Test vs. Control)**
- **Result:** The Test group showed a significantly higher completion rate.  
- **Conclusion:** The UI redesign positively impacted user completion.

### **2. Cost-Effectiveness Threshold (â‰¥ 5% improvement)**
- **Result:** Improvement exceeded Vanguardâ€™s 5% minimum threshold.  
- **Conclusion:** Redesign is both effective and cost-efficient.

### **3. Demographic Differences (Age, Tenure, Gender)**
- **Result:** No significant differences across groups.  
- **Conclusion:** Test and Control groups were balanced; results not driven by demographic skew.

---

## ğŸ§­ Experiment Evaluation
- **Design Quality:** Random assignment ensured unbiased comparison.  
- **Duration:** The experiment ran from **3/15/2017 to 6/20/2017**, providing sufficient data.  
- **Additional Data Recommended:**
  - User surveys for qualitative insights  
  - Post-experiment follow-up to measure long-term behavior  
  - More granular segmentation for targeted insights  

---

## ğŸ Conclusion
The redesigned UI demonstrated clear strengths:

### âœ” Significant improvement in completion rate  
### âœ” Improvement exceeded the 5% cost-effectiveness threshold  
### âœ” Cleaner, faster overall process experience  

However, two areas require further optimization:
- **Steps 2 and 3** showed slower performance and could be streamlined.  
- **Higher error rates** in the Test group suggest possible UI friction.

Overall, the redesign is impactful, but refinement will further enhance user experience and process efficiency.

---

